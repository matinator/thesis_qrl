\section{Conclusion}
\subsection{Results obtained}
So in this thesis after a first dedicated to introducing the basis of \acrlong{drl} and quantum mechanics it was later presented the \acrlong{vqa} to simulate and replace \acrlong{nn}, observing what kind of advantage was possible to obtain in the context of reinforcement learning.
To understand if there was a quantistic advantage and on which aspect was present the ansatz proposed in the paper \cite{Scholik_2022} was tested as a quantum variation of the \acrlong{dqn} on Cartpole environment.\\
To compare it, a model with neural networks was run with the most number of hyperparameters equal between the two. From results obtained and plot \ref{fig:vqcnncomparisonmedia}, it was demonstrated that an effective advantage was obtained: an impressive smaller number of parameters was required to reach the objective and a neural network with the same number of weights was unable to do it.\\
This suggested that there is a more powerful expressivity and approximation capability compared to the neural network in this case. Unfortunately, such kind of environment is extremely simple to solve and doesn't completely demonstrate this kind of ability and extension to more complex cases.\\
For this reason and to use the possible advantages and applications in the industry a different environment was tested: the robotic arm. This required changing the algorithm used previously due to continuous state and actions which the \acrshort{dqn} is unable to deal for its discrete nature.\\
The new algorithm that was used is called \acrfull{sac}, differently from the \acrshort{dqn} it is policy gradient-based and uses 5 components to learn. These are actor policy which must calculate the mean and standard deviation, 2 action values and 2 target action values that must update policy and approximate the correct functions to learn and work.
From a previous paper, \cite{https://doi.org/10.48550/arxiv.2112.11921} it was defined as a partial quantum \acrshort{sac} which demonstrated an advantage on the number of parameters by using a hybrid actor component. From the test conducted later on and in particular from plot \ref{fig:vqa-critic-reduced} an important conclusion was extrapolated: the critical component that determines the performance of the algorithm is the action-values components.
So for this conclusion, another run using all components as a hybrid was conducted with an additional classical layer for the critical components to introduce enough non-linearity. Furthermore, more tests using all components leveraging neural networks were run increasing every time the dimension of neurons on hidden layers.
Confronting the trend on plot \ref{fig:best-classical-vs-all-hybrid-results} it was noticed that to have a similar trend for the classical and hybrid run, several more parameters between 7 to 100 times and not always the steps required needed to be similar. 
Furthermore running an example where all components using the neural network have the same parameters number of hybrid, this model was unable to reach the goal of environment suggesting a quantum advantage on the number of parameters required and ability of expressivity useful for industrial applications.
\subsection{Future directions}
Possible future directions that should be interesting to take are:
\begin{itemize}
	\item applies multiple runs on the hybrid algorithm possibly reducing the time required to run.
	\item introduces the noise on algorithms execution to understand the possible non-linearity and performance impacts on the ability to learn and the difference in weights.
	\item develops better ansatz for the \acrshort{vqa} to reduce the dependence of non-linearity by neural network layers and minimize the depth of the circuit.
	\item test this \acrshort{vqa} on a real quantum computing using the parameter-shift method to calculate the gradients and use it later on a CPU to optimize the parameters showing performance and time reduction compared to this run on a simulator.
	\item tests these algorithms on multiple environments to see if are better or worse compared to "classical" ones.
	\item extending the introduction of \acrshort{vqa} on other algorithms of \acrshort{drl} such as \acrshort{ppo}, \acrshort{ddpg} and others to confirm quantum advantage.
\end{itemize}