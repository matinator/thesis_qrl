\section{Conclusion}
\subsection{Results obtained}
So in this thesis after a first dedicated to introduce the basis of \acrlong{drl} and quantum mechanics it was later presented the \acrlong{vqa} to simulate and replace \acrlong{nn}, observing what kind of advantage was possible to obtain in the context of reinforcement learning.
In order to understand if there was a quantistic advantage and on which aspect was present the ansatz proposed in the paper \cite{Scholik_2022} was tested as a quantum variation of the \acrlong{dqn} on Cartpole environment.\\
In order to compare it, a model with neural networks was runned with the most number of hyperparameters equal between the two. From results obtained and plot \ref{fig:vqcnncomparisonmedia} it was demonstrated that an effective advantage was obtained: an impressive smaller number of parameters was required to reach the objective and a neural network with the same number of weights was unable to do it.\\
This suggested that there is a more powerful expressivity and approximation capability compared to the neural network in this case. Unfortunately such kind of environment is extremely simple to solve and doesn't completely demonstrate this kind of ability and extension to more complex cases.\\
For this reason and to use the possible advantages and application on the industry a different  environment was tested: the robotic arm. This required to change the algorithm used previously due to continous state and actions which the \acrshort{dqn} is unable to deal for its discrete nature.\\
The new algorithm that was used is called \acrfull{sac}, differently from the \acrshort{dqn} it is policy gradient based and uses 5 components to learn. These are: actor policy which must calculate the mean and standard deviation, 2 action-values and 2 target action values that must update policy and approximate the correct functions in order to learn and work.
From a previous paper \cite{https://doi.org/10.48550/arxiv.2112.11921} it was defined a partial quantum \acrshort{sac} which demonstrated an advantage on the number of parameters by using an hybrid actor component. From the test conducted later on and in particular from plot \ref{fig:vqa-critic-reduced} an important conclusion was extrapolated: the critical component  that determines the performance of algorithm are action-values components.
So for this conclusion another run using all component as hybrid was conducted with an additional classical layer for the critical components in order to introduce enough non-linearity. Furthermore more test using all components leveraging neural networks were runned increasing every time the dimension of neurons on hidden layers.
Confronting the trend on plott \ref{fig:best-classical-vs-all-hybrid-results} it was noticed that in order to have a similar trend for the classical and hybrid run, it was necessary a number of parameters between 7 to 100 times and not always the steps required was similar. 
Furthermore running an example were all components using neural network has the same parameters number of hybrid, this model was unable to reach the goal of environment suggesting a quantum advantage on the number of parameters required and ability of expressivity useful for industrial applications.
\subsection{Future directions}
Possible future directions that should be interesting to take are:
 \begin{itemize}
	\item applies multiple runs on the hybrid algorithm possibly reducing the time required to run.
	\item introduce the noise on algorithms execution to understand the possible non-linearity and performance impacts on the ability to learn and difference on weights.
	\item develop better ansatz for the \acrshort{vqa} in order to reduce the dependence of non-linearity by neural networks layers and minimize the depth of circuit.
	\item test this \acrshort{vqa} on a real quantum computing using the parameter-shift method to calculate the gradients and use it later on a CPU to optimize the parameters showing performance and time reduction compared to this run on a simulators.
	\item test this algorithms on multiple environments to see if are better or worse compared to "classical" ones.
	\item extending the introduction of \acrshort{vqa} on other algorithms of \acrshort{drl} such as PPO, DDPG to confirm quantum advantage.
\end{itemize}