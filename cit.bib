@article{q-learning,
	title = {Q-learning},
	author = {Watkins, C.J.C.H., Dayan},
	journal = {Mach Learn},
	volume = {8},
	pages = {279–292},
	year = {1992},
	DOI = {https://doi.org/10.1007/BF00992698},
	keywords = {neural networks},
}

@article{Rosenblatt1958ThePA,
	title={The perceptron: a probabilistic model for information storage and organization in the brain.},
	author={Frank Rosenblatt},
	journal={Psychological review},
	year={1958},
	volume={65 6},
	pages={386-408},
	keywords = {neural networks},
}

@article{backpropagation,
	author = {David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams},
	title = {Learning representations by back-propagating errors},
	journal = {Nature},
	volume = {323},
	pages = {533–536},
	year = {1986},
	DOI = {https://doi.org/10.1038/323533a0},
	keywords = {neural networks},
}

@article{universals,
	author = {Kurt Hornik,Maxwell Stinchcombe,Halbert White},
	title = {Multilayer feedforward networks are universal approximators},
	journal = {Neural Networks},
	volume = {5 2},
	pages = {359-366},
	year = {1989},
	DOI = {https://doi.org/10.1016/0893-6080(89)90020-8},
	keywords = {neural networks},
}

@article{DBLP:journals/corr/MnihKSGAWR13,
	author    = {Volodymyr Mnih and
	Koray Kavukcuoglu and
	David Silver and
	Alex Graves and
	Ioannis Antonoglou and
	Daan Wierstra and
	Martin A. Riedmiller},
	title     = {Playing Atari with Deep Reinforcement Learning},
	journal   = {CoRR},
	volume    = {abs/1312.5602},
	year      = {2013},
	url       = {http://arxiv.org/abs/1312.5602},
	eprinttype = {arXiv},
	eprint    = {1312.5602},
	timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/HasseltGS15,
	author    = {Hado van Hasselt and
	Arthur Guez and
	David Silver},
	title     = {Deep Reinforcement Learning with Double Q-learning},
	journal   = {CoRR},
	volume    = {abs/1509.06461},
	year      = {2015},
	url       = {http://arxiv.org/abs/1509.06461},
	eprinttype = {arXiv},
	eprint    = {1509.06461},
	timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HasseltGS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.5555/3009657.3009806,
	author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
	title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
	year = {1999},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
	booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
	pages = {1057–1063},
	numpages = {7},
	location = {Denver, CO},
	series = {NIPS'99}
}

@article{DBLP:journals/corr/WangBHMMKF16,
	author    = {Ziyu Wang and
	Victor Bapst and
	Nicolas Heess and
	Volodymyr Mnih and
	R{\'{e}}mi Munos and
	Koray Kavukcuoglu and
	Nando de Freitas},
	title     = {Sample Efficient Actor-Critic with Experience Replay},
	journal   = {CoRR},
	volume    = {abs/1611.01224},
	year      = {2016},
	url       = {http://arxiv.org/abs/1611.01224},
	eprinttype = {arXiv},
	eprint    = {1611.01224},
	timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/WangBHMMKF16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SchulmanWDRK17,
	author    = {John Schulman and
	Filip Wolski and
	Prafulla Dhariwal and
	Alec Radford and
	Oleg Klimov},
	title     = {Proximal Policy Optimization Algorithms},
	journal   = {CoRR},
	volume    = {abs/1707.06347},
	year      = {2017},
	url       = {http://arxiv.org/abs/1707.06347},
	eprinttype = {arXiv},
	eprint    = {1707.06347},
	timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1801-01290,
	author    = {Tuomas Haarnoja and
	Aurick Zhou and
	Pieter Abbeel and
	Sergey Levine},
	title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
	with a Stochastic Actor},
	journal   = {CoRR},
	volume    = {abs/1801.01290},
	year      = {2018},
	url       = {http://arxiv.org/abs/1801.01290},
	eprinttype = {arXiv},
	eprint    = {1801.01290},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}