@article{q-learning,
	title = {Q-learning},
	author = {Watkins, C.J.C.H., Dayan},
	journal = {Mach Learn},
	volume = {8},
	pages = {279–292},
	year = {1992},
	DOI = {https://doi.org/10.1007/BF00992698},
	keywords = {neural networks},
}

@article{Rosenblatt1958ThePA,
	title={The perceptron: a probabilistic model for information storage and organization in the brain.},
	author={Frank Rosenblatt},
	journal={Psychological review},
	year={1958},
	volume={65 6},
	pages={386-408},
	keywords = {neural networks},
}

@article{backpropagation,
	author = {David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams},
	title = {Learning representations by back-propagating errors},
	journal = {Nature},
	volume = {323},
	pages = {533–536},
	year = {1986},
	DOI = {https://doi.org/10.1038/323533a0},
	keywords = {neural networks},
}

@article{universals,
	author = {Kurt Hornik,Maxwell Stinchcombe,Halbert White},
	title = {Multilayer feedforward networks are universal approximators},
	journal = {Neural Networks},
	volume = {5 2},
	pages = {359-366},
	year = {1989},
	DOI = {https://doi.org/10.1016/0893-6080(89)90020-8},
	keywords = {neural networks},
}

@article{DBLP:journals/corr/MnihKSGAWR13,
	author    = {Volodymyr Mnih and
	Koray Kavukcuoglu and
	David Silver and
	Alex Graves and
	Ioannis Antonoglou and
	Daan Wierstra and
	Martin A. Riedmiller},
	title     = {Playing Atari with Deep Reinforcement Learning},
	journal   = {CoRR},
	volume    = {abs/1312.5602},
	year      = {2013},
	url       = {http://arxiv.org/abs/1312.5602},
	eprinttype = {arXiv},
	eprint    = {1312.5602},
	timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/HasseltGS15,
	author    = {Hado van Hasselt and
	Arthur Guez and
	David Silver},
	title     = {Deep Reinforcement Learning with Double Q-learning},
	journal   = {CoRR},
	volume    = {abs/1509.06461},
	year      = {2015},
	url       = {http://arxiv.org/abs/1509.06461},
	eprinttype = {arXiv},
	eprint    = {1509.06461},
	timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HasseltGS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.5555/3009657.3009806,
	author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
	title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
	year = {1999},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
	booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
	pages = {1057–1063},
	numpages = {7},
	location = {Denver, CO},
	series = {NIPS'99}
}

@article{DBLP:journals/corr/WangBHMMKF16,
	author    = {Ziyu Wang and
	Victor Bapst and
	Nicolas Heess and
	Volodymyr Mnih and
	R{\'{e}}mi Munos and
	Koray Kavukcuoglu and
	Nando de Freitas},
	title     = {Sample Efficient Actor-Critic with Experience Replay},
	journal   = {CoRR},
	volume    = {abs/1611.01224},
	year      = {2016},
	url       = {http://arxiv.org/abs/1611.01224},
	eprinttype = {arXiv},
	eprint    = {1611.01224},
	timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/WangBHMMKF16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SchulmanWDRK17,
	author    = {John Schulman and
	Filip Wolski and
	Prafulla Dhariwal and
	Alec Radford and
	Oleg Klimov},
	title     = {Proximal Policy Optimization Algorithms},
	journal   = {CoRR},
	volume    = {abs/1707.06347},
	year      = {2017},
	url       = {http://arxiv.org/abs/1707.06347},
	eprinttype = {arXiv},
	eprint    = {1707.06347},
	timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1801-01290,
	author    = {Tuomas Haarnoja and
	Aurick Zhou and
	Pieter Abbeel and
	Sergey Levine},
	title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
	with a Stochastic Actor},
	journal   = {CoRR},
	volume    = {abs/1801.01290},
	year      = {2018},
	url       = {http://arxiv.org/abs/1801.01290},
	eprinttype = {arXiv},
	eprint    = {1801.01290},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.2307/2276774,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/2276774},
	author = {Edwin B. Wilson},
	journal = {Journal of the American Statistical Association},
	number = {158},
	pages = {209--212},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Probable Inference, the Law of Succession, and Statistical Inference},
	urldate = {2022-06-17},
	volume = {22},
	year = {1927}
}

@Inbook{Schuld2021,
	author={Schuld, Maria
	and Petruccione, Francesco},
	title={Representing Data on a Quantum Computer},
	bookTitle={Machine Learning with Quantum Computers},
	year={2021},
	publisher={Springer International Publishing},
	address={Cham},
	pages={147--176},
	abstract={This chapter presents one of the most important parts of quantum machine learning algorithms, namely strategies with which a single data point, or sometimes an entire dataset, can be encoded into quantum states. We present quantum routines for this task, discuss their runtimes and review their interpretation as feature maps known in classical machine learning.},
	isbn={978-3-030-83098-4},
	doi={10.1007/978-3-030-83098-4_4},
	url={https://doi.org/10.1007/978-3-030-83098-4_4}
}

@article{Cerezo_2021,
	doi = {10.1038/s42254-021-00348-9},
	
	url = {https://doi.org/10.1038%2Fs42254-021-00348-9},
	
	year = 2021,
	month = {8},
	
	publisher = {Springer Science and Business Media {LLC}
	},
	
	volume = {3},
	
	number = {9},
	
	pages = {625--644},
	
	author = {M. Cerezo and Andrew Arrasmith and Ryan Babbush and Simon C. Benjamin and Suguru Endo and Keisuke Fujii and Jarrod R. McClean and Kosuke Mitarai and Xiao Yuan and Lukasz Cincio and Patrick J. Coles},
	
	title = {Variational quantum algorithms},
	
	journal = {Nature Reviews Physics}
}

@misc{https://doi.org/10.48550/arxiv.1802.06002,
	doi = {10.48550/ARXIV.1802.06002},
	
	url = {https://arxiv.org/abs/1802.06002},
	
	author = {Farhi, Edward and Neven, Hartmut},
	
	keywords = {Quantum Physics (quant-ph), FOS: Physical sciences, FOS: Physical sciences},
	
	title = {Classification with Quantum Neural Networks on Near Term Processors},
	
	publisher = {arXiv},
	
	year = {2018},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Benedetti_2019,
	doi = {10.1038/s41534-019-0157-8},
	
	url = {https://doi.org/10.1038%2Fs41534-019-0157-8},
	
	year = 2019,
	month = {5},
	
	publisher = {Springer Science and Business Media {LLC}
	},
	
	volume = {5},
	
	number = {1},
	
	author = {Marcello Benedetti and Delfina Garcia-Pintos and Oscar Perdomo and Vicente Leyton-Ortega and Yunseong Nam and Alejandro Perdomo-Ortiz},
	
	title = {A generative modeling approach for benchmarking and training shallow quantum circuits},
	
	journal = {npj Quantum Information}
}

@article{McClean_2018,
	doi = {10.1038/s41467-018-07090-4},
	
	url = {https://doi.org/10.1038%2Fs41467-018-07090-4},
	
	year = 2018,
	month = {11},
	
	publisher = {Springer Science and Business Media {LLC}
	},
	
	volume = {9},
	
	number = {1},
	
	author = {Jarrod R. McClean and Sergio Boixo and Vadim N. Smelyanskiy and Ryan Babbush and Hartmut Neven},
	
	title = {Barren plateaus in quantum neural network training landscapes},
	
	journal = {Nature Communications}
}

@article{Grant_2019,
	doi = {10.22331/q-2019-12-09-214},
	
	url = {https://doi.org/10.22331%2Fq-2019-12-09-214},
	
	year = 2019,
	month = {12},
	
	publisher = {Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
	
	volume = {3},
	
	pages = {214},
	
	author = {Edward Grant and Leonard Wossnig and Mateusz Ostaszewski and Marcello Benedetti},
	
	title = {An initialization strategy for addressing barren plateaus in parametrized quantum circuits},
	
	journal = {Quantum}
}

@article{Schuld_theorem,
	doi = {10.1103/physreva.103.032430},
	url = {https://doi.org/10.1103%2Fphysreva.103.032430},
	year = 2021,
	moth = {3},
	publisher = {American Physical Society ({APS})},
	volume = {103},
	number = {3},
	author = {Maria Schuld and Ryan Sweke and Johannes Jakob Meyer},
	title = {Effect of data encoding on the expressive power of variational quantum-machine-learning models},
	journal = {Physical Review A}
}

@article{P_rez_Salinas_2020,
	doi = {10.22331/q-2020-02-06-226},
	url = {https://doi.org/10.22331%2Fq-2020-02-06-226},
	year = 2020,
	month = {2},
	publisher = {Verein zur Forderung des Open Access Publizierens in den Quantenwissenschafften},
	volume = {4},
	pages = {226},
	author = {Adri{\'{a}}n P{\'{e}}rez-Salinas and Alba Cervera-Lierta and Elies Gil-Fuster and Jos{\'{e}}I. Latorre},
	title = {Data re-uploading for a univeral quantum classifier},
	journal = {Quantum}
}

@misc{1606.01540,
	author = {Greg Brockman and Vicki Cheung and Ludwing Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
	title = {OpeanAI Gym},
	year = {2016},
	Eprint = {arXiv:1606.01540}
}

@article{Scholik_2022,
	doi = {10.22331/q-2022-05-24-720},
	url = {https://doi.org/10.22331%2Fq-2022-05-24-720},
	year = 2022,
	month = {5},
	publisher = {Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
	volume = {6},
	pages = {720},
	author = {Andrea Skolik and Sofiene Jerbi and Vedran Dunjko},
	title = {Quantum agents in the Gym: a variational quantum  algorithm for deep Q-learning},
	journal = {Quantum}
}

@Inbook{Schuld2021vqa,
	author="Schuld, Maria
	and Petruccione, Francesco",
	title="Variational Circuits as Machine Learning Models",
	bookTitle="Machine Learning with Quantum Computers",
	year="2021",
	publisher="Springer International Publishing",
	address="Cham",
	pages="177--215",
	abstract="We explain how parametrised quantum circuits---quantum algorithms that are popular in near-term quantum computing---can be used as machine learning models, and review techniques to analyse and train such quantum models in a deep-learning fashion, including measures of expressivity and trainability, as well as parameter-shift rules.",
	isbn="978-3-030-83098-4",
	doi="10.1007/978-3-030-83098-4_5",
	url="https://doi.org/10.1007/978-3-030-83098-4_5"
}

@misc{https://doi.org/10.48550/arxiv.2112.11921,
	doi = {10.48550/ARXIV.2112.11921},
	
	url = {https://arxiv.org/abs/2112.11921},
	
	author = {Lan, Qingfeng},
	
	keywords = {Quantum Physics (quant-ph), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Variational Quantum Soft Actor-Critic},
	
	publisher = {arXiv},
	
	year = {2021},
	
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@book{10.5555/3279266,
	author = {Lapan, Maxim},
	title = {Deep Reinforcement Learning Hands-On: Apply Modern RL Methods, with Deep Q-Networks, Value Iteration, Policy Gradients, TRPO, AlphaGo Zero and More},
	year = {2018},
	isbn = {1788834240},
	publisher = {Packt Publishing},
	abstract = {This practical guide will teach you how deep learning (DL) can be used to solve complex real-world problems. Key Features Explore deep reinforcement learning (RL), from the first principles to the latest algorithms Evaluate high-profile RL methods, including value iteration, deep Q-networks, policy gradients, TRPO, PPO, DDPG, D4PG, evolution strategies and genetic algorithms Keep up with the very latest industry developments, including AI-driven chatbots Book Description Recent developments in reinforcement learning (RL), combined with deep learning (DL), have seen unprecedented progress made towards training agents to solve complex problems in a human-like way. Google's use of algorithms to play and defeat the well-known Atari arcade games has propelled the field to prominence, and researchers are generating new ideas at a rapid pace. Deep Reinforcement Learning Hands-On is a comprehensive guide to the very latest DL tools and their limitations. You will evaluate methods including Cross-entropy and policy gradients, before applying them to real-world environments. Take on both the Atari set of virtual games and family favorites such as Connect4. The book provides an introduction to the basics of RL, giving you the know-how to code intelligent learning agents to take on a formidable array of practical tasks. Discover how to implement Q-learning on 'grid world' environments, teach your agent to buy and trade stocks, and find out how natural language models are driving the boom in chatbots. What you will learn Understand the DL context of RL and implement complex DL modelsLearn the foundation of RL: Markov decision processes Evaluate RL methods including Cross-entropy, DQN, Actor-Critic, TRPO, PPO, DDPG, D4PG and others Discover how to deal with discrete and continuous action spaces in various environments Defeat Atari arcade games using the value iteration method Create your own Open AI Gym environment to train a stock trading agent Teach your agent to play Connect 4 using AlphaGo Zero Explore the very latest deep RL research on topics including AI-driven chatbots Who This Book Is For Some fluency in Python is assumed. Basic deep learning (DL) approaches should be familiar to readers and some practical experience in DL will be helpful. This book is an introduction to deep reinforcement learning (RL) and requires no background in RL.}
}

@book{nielsen_chuang_2010, 
	place={Cambridge}, 
	title={Quantum Computation and Quantum Information: 10th Anniversary Edition}, DOI={10.1017/CBO9780511976667}, 
	publisher={Cambridge University Press}, 
	author={Nielsen, Michael A. and Chuang, Isaac L.}, 
	year={2010}}