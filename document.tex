\documentclass[12pt,a4paper]{article}
\newtheorem{mydef}{Definition}
\newtheorem{theorem}{Theorem}
%packages
\usepackage[dvipsnames]{xcolor}
\usepackage{braket}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{mathtools}
\usetikzlibrary{quantikz}
\usetikzlibrary{arrows.meta}
\usepackage[acronym]{glossaries}
\usepackage[backend=biber,
style=numeric,
citestyle=phys,
]{biblatex} 
\addbibresource{cit.bib}
\usetikzlibrary{positioning,angles,quotes}
\def\layersep{2.5cm}
%glossary 
\makeglossaries
\newacronym{qrl}{QRL}{Quantum Reinforcement Learning}
\newacronym{ai}{AI}{Artificial Intelligence}
\newacronym{drl}{DRL}{Deep Reinforcement Learning}
\newacronym{rl}{RL}{Reinforcement Learning}
\newacronym{qc}{QC}{Quantum Computing}
\newacronym{nn}{NN}{Neural Networks}
\newacronym{mdp}{MDP}{Markov Decision Process}
\newacronym{vqa}{VQA}{Variational Quantum Algorithm}
\newacronym{iid}{i.i.d.}{indipendent identical distributed}
\newacronym{dqn}{DQN}{Deep Q-Network}
\newacronym{sac}{SAC}{Soft-Actor Critic}
\newacronym{a2c}{A2C}{Actor-Critic}
\newacronym{ppo}{PPO}{Proximal Policy Optimization}
\newacronym{qpu}{QPU}{Quantum Processing Unit}
\newacronym{ddpg}{DDPG}{Deep Deterministic Policy Gradient}
%settings
\usepackage[left=3.6cm, bottom=4cm, right=3.6cm, top=4cm]{geometry}
\author{Matteo Conterno}
\title{Prospects of quantum computing approach to reinforcement learning}
%start of document
\begin{document}
	\include{chapters/titlepage}
	
	\include{chapters/abstract}
	{
	 	\hypersetup{hidelinks}
		\tableofcontents
	}
    \newpage
	\printglossary[type=\acronymtype]
	\include{chapters/Introduction}
	\include{chapters/vqa_reinforcement}
	\include{chapters/conclusion}
	\printbibliography[
	heading=bibintoc,
	title={Bibliography}
	]
\end{document}